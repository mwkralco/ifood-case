# NYC Taxi Data Pipeline

Este projeto implementa um **pipeline de dados** baseado na modelagem **Medalh√£o (Bronze, Silver, Gold)**, utilizando:

‚úÖ **Apache Spark** no **Databricks Free Tier**  
‚úÖ **Armazenamento em Parquet** diretamente no **AWS S3 Free Tier**

O objetivo √© processar os dados p√∫blicos de corridas de t√°xi de Nova York, organizando-os em camadas que melhoram a **estrutura**, **qualidade** e **efici√™ncia** das an√°lises.

## Tecnologias Utilizadas

- **Apache Spark** (via Databricks Community Edition - gratuito)
- **AWS S3** (Free Tier)
- **Parquet** como formato de armazenamento
- **boto3** para intera√ß√µes com S3
- **pandas** para manipula√ß√£o tabular
- **requests** para APIs externas

## Estrutura do Pipeline ‚Äî Modelagem Medalh√£o

O pipeline segue a modelagem **Medalh√£o**, uma arquitetura de refer√™ncia que organiza o fluxo de dados em tr√™s camadas:

### ü•â Bronze ‚Äî Dados Brutos

- Armazenamento fiel dos dados recebidos, em formato **Parquet**.
- Nenhuma transforma√ß√£o √© aplicada, garantindo **rastreabilidade** e possibilidade de **reprocessamento**.

### ü•à Silver ‚Äî Dados Limpos

- Transforma√ß√µes b√°sicas: tipagens corretas, cria√ß√£o de colunas auxiliares e parti√ß√µes para consultas eficientes.
- Filtragem de inconsist√™ncias.

### ü•á Gold ‚Äî Dados Filtrados

- Somente os dados relevantes para **an√°lise**.
- Otimiza o consumo por **dashboards**, **relat√≥rios** e **Data Science**.

## Por que usar a modelagem Medalh√£o?

‚úÖ **Rastreabilidade:** cada camada documenta uma etapa do processamento.  
‚úÖ **Qualidade:** permite transformar, validar e enriquecer os dados em etapas.  
‚úÖ **Efici√™ncia:** consultas anal√≠ticas podem ser feitas diretamente na camada **Gold**, mais enxuta e otimizada.  
‚úÖ **Manuten√ß√£o:** facilita **reprocessamentos parciais** sem afetar dados finais.

## Configura√ß√£o

### Requisitos

- Conta gratuita no **Databricks Community Edition**
- **AWS S3** configurado (Free Tier)
- **Python** >= 3.7
- **Spark** >= 3.0

### Passos

1. Configure as credenciais AWS no Databricks:  
   - AWS_ACCESS_KEY_ID 
   - AWS_SECRET_ACCESS_KEY

2. Ajuste as vari√°veis no c√≥digo:  
   - bucket: nome do bucket S3  
   - year_val: ano desejado  
   - months: lista de meses a processar

## Armazenamento no S3

O pipeline cria as seguintes estruturas:
- s3a://<bucket>/bronze/yellow_tripdata/year=<YYYY>/month=<MM>/
- s3a://<bucket>/silver/yellow_tripdata/year=<YYYY>/month=<MM>/
- s3a://<bucket>/gold/yellow_tripdata/year=<YYYY>/month=<MM>/

Todas as camadas armazenam arquivos **Parquet**, aproveitando a **compacta√ß√£o** e a **leitura eficiente** desse formato.


## Exemplo de consulta anal√≠tica no Spark SQL

```sql
SELECT 
    HOUR(tpep_pickup_datetime) AS pickup_hour,
    AVG(passenger_count) AS avg_passengers
FROM yellow_tripdata
WHERE year = 2023 AND month = 5
GROUP BY 1
ORDER BY 1;
```
# Autor
Desenvolvido por Mariana Kralco.



